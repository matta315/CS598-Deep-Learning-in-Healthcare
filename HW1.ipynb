{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1\n",
    "\n",
    "## Overview\n",
    "\n",
    "Preparing the data, computing basic statistics and constructing simple models are essential steps for data science practice. In this homework, you will use clinical data as raw input to perform **Heart Failure Prediction**. For this homework, **Python** programming will be required. See the attached skeleton code as a start-point for the programming questions.\n",
    "\n",
    "This homework assumes familiarity with Pandas. If you need a Pandas crash course, we recommend working through [100 Pandas Puzzles](https://github.com/ajcr/100-pandas-puzzles), the solutions are also available at that link. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "DATA_PATH = \"../HW1-lib/data/\"\n",
    "TRAIN_DATA_PATH = DATA_PATH + \"train/\"\n",
    "VAL_DATA_PATH = DATA_PATH + \"val/\"\n",
    "    \n",
    "sys.path.append(\"../HW1-lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "DATA_PATH = \"../HW1-lib/data/\"\n",
    "TRAIN_DATA_PATH = DATA_PATH + \"train/\"\n",
    "VAL_DATA_PATH = DATA_PATH + \"val/\"\n",
    "    \n",
    "sys.path.append(\"../HW1-lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "DATA_PATH = \"../HW1-lib/data/\"\n",
    "TRAIN_DATA_PATH = DATA_PATH + \"train/\"\n",
    "VAL_DATA_PATH = DATA_PATH + \"val/\"\n",
    "    \n",
    "sys.path.append(\"../HW1-lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "DATA_PATH = \"../HW1-lib/data/\"\n",
    "TRAIN_DATA_PATH = DATA_PATH + \"train/\"\n",
    "VAL_DATA_PATH = DATA_PATH + \"val/\"\n",
    "    \n",
    "sys.path.append(\"../HW1-lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "DATA_PATH = \"../HW1-lib/data/\"\n",
    "TRAIN_DATA_PATH = DATA_PATH + \"train/\"\n",
    "VAL_DATA_PATH = DATA_PATH + \"val/\"\n",
    "    \n",
    "sys.path.append(\"../HW1-lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "DATA_PATH = \"../HW1-lib/data/\"\n",
    "TRAIN_DATA_PATH = DATA_PATH + \"train/\"\n",
    "VAL_DATA_PATH = DATA_PATH + \"val/\"\n",
    "    \n",
    "sys.path.append(\"../HW1-lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "DATA_PATH = \"../HW1-lib/data/\"\n",
    "TRAIN_DATA_PATH = DATA_PATH + \"train/\"\n",
    "VAL_DATA_PATH = DATA_PATH + \"val/\"\n",
    "    \n",
    "sys.path.append(\"../HW1-lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Raw Data\n",
    "\n",
    "For this homework, we will be using a clinical dataset synthesized from [MIMIC-III](https://www.nature.com/articles/sdata201635).\n",
    "\n",
    "Navigate to `TRAIN_DATA_PATH`. There are three CSV files which will be the input data in this homework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $TRAIN_DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**events.csv**\n",
    "\n",
    "The data provided in *events.csv* are event sequences. Each line of this file consists of a tuple with the format *(pid, event_id, vid, value)*. \n",
    "\n",
    "For example, \n",
    "\n",
    "```\n",
    "33,DIAG_244,0,1\n",
    "33,DIAG_414,0,1\n",
    "33,DIAG_427,0,1\n",
    "33,LAB_50971,0,1\n",
    "33,LAB_50931,0,1\n",
    "33,LAB_50812,1,1\n",
    "33,DIAG_425,1,1\n",
    "33,DIAG_427,1,1\n",
    "33,DRUG_0,1,1\n",
    "33,DRUG_3,1,1\n",
    "```\n",
    "\n",
    "- **pid**: De-identified patient identier. For example, the patient in the example above has pid 33. \n",
    "- **event_id**: Clinical event identifier. For example, DIAG_244 means the patient was diagnosed of disease with ICD9 code [244](http://www.icd9data.com/2013/Volume1/240-279/240-246/244/244.htm); LAB_50971 means that the laboratory test with code 50971 was conducted on the patient; and DRUG_0 means that a drug with code 0 was prescribed to the patient. Corresponding lab (drug) names can be found in `{DATA_PATH}/lab_list.txt` (`{DATA_PATH}/drug_list.txt`).\n",
    "- **vid**: Visit identifier. For example, the patient has two visits in total. Note that vid is ordinal. That is, visits with bigger vid occour after that with smaller vid.\n",
    "- **value**: Contains the value associated to an event (always 1 in the synthesized dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hf_events.csv**\n",
    "\n",
    "The data provided in *hf_events.csv* contains pid of patients who have been diagnosed with heart failure (i.e., DIAG_398, DIAG_402, DIAG_404, DIAG_428) in at least one visit. They are in the form of a tuple with the format *(pid, vid, label)*. For example,\n",
    "\n",
    "```\n",
    "156,0,1\n",
    "181,1,1\n",
    "```\n",
    "\n",
    "The vid indicates the index of the first visit with heart failure of that patient and a label of 1 indicates the presence of heart failure. **Note that only patients with heart failure are included in this file. Patients who are not mentioned in this file have never been diagnosed with heart failure.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**event_feature_map.csv**\n",
    "\n",
    "The *event_feature_map.csv* is a map from an event_id to an integer index. This file contains *(idx, event_id)* pairs for all event ids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Descriptive Statistics [20 points]\n",
    "\n",
    "Before starting analytic modeling, it is a good practice to get descriptive statistics of the input raw data. In this question, you need to write code that computes various metrics on the data described previously. A skeleton code is provided to you as a starting point.\n",
    "\n",
    "The definition of terms used in the result table are described below:\n",
    "\n",
    "- **Event count**: Number of events recorded for a given patient.\n",
    "- **Encounter count**: Number of visits recorded for a given patient.\n",
    "\n",
    "Note that every line in the input file is an event, while each visit consists of multiple events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete the following code cell to implement the required statistics.**\n",
    "\n",
    "Please be aware that **you are NOT allowed to change the filename and any existing function declarations.** Only `numpy`, `scipy`, `scikit-learn`, `pandas` and other built-in modules of python will be available for you to use. The use of `pandas` library is suggested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# PLEASE USE THE GIVEN FUNCTION NAME, DO NOT CHANGE IT.\n",
    "\n",
    "def read_csv(filepath=TRAIN_DATA_PATH):\n",
    "\n",
    "    '''\n",
    "    Read the events.csv and hf_events.csv files. \n",
    "    Variables returned from this function are passed as input to the metric functions.\n",
    "    \n",
    "    NOTE: remember to use `filepath` whose default value is `TRAIN_DATA_PATH`.\n",
    "    '''\n",
    "    \n",
    "    events = pd.read_csv(filepath + 'events.csv')\n",
    "    hf = pd.read_csv(filepath + 'hf_events.csv')\n",
    "\n",
    "    return events, hf\n",
    "\n",
    "def event_count_metrics(events, hf):\n",
    "\n",
    "    '''\n",
    "    TODO : Implement this function to return the event count metrics.\n",
    "    \n",
    "    Event count is defined as the number of events recorded for a given patient.\n",
    "    '''\n",
    "    ## your code here\n",
    "    \n",
    "    # Count events per patient\n",
    "    event_counts = events['pid'].value_counts().reset_index()\n",
    "    event_counts.columns = ['pid', 'event_count']\n",
    "    \n",
    "    # Merge event counts with HF status\n",
    "    patient_df = event_counts.merge(hf, on='pid', how='left')\n",
    "    normal_patients = patient_df[patient_df['label'].isna()]\n",
    "    hf_patients = patient_df[patient_df['label']==1]\n",
    "    \n",
    "    # Calculate metrics for HF patients\n",
    "    avg_hf_event_count = hf_patients['event_count'].mean() if not hf_patients.empty else None\n",
    "    max_hf_event_count = hf_patients['event_count'].max() if not hf_patients.empty else None\n",
    "    min_hf_event_count = hf_patients['event_count'].min() if not hf_patients.empty else None\n",
    "    \n",
    "    # Calculate metrics for normal patients\n",
    "    avg_norm_event_count = normal_patients['event_count'].mean() if not normal_patients.empty else None\n",
    "    max_norm_event_count = normal_patients['event_count'].max() if not normal_patients.empty else None\n",
    "    min_norm_event_count = normal_patients['event_count'].min() if not normal_patients.empty else None\n",
    "\n",
    "    return avg_hf_event_count, max_hf_event_count, min_hf_event_count, \\\n",
    "           avg_norm_event_count, max_norm_event_count, min_norm_event_count\n",
    "\n",
    "def encounter_count_metrics(events, hf):\n",
    "\n",
    "    '''\n",
    "    TODO : Implement this function to return the encounter count metrics.\n",
    "    \n",
    "    Encounter count is defined as the number of visits recorded for a given patient. \n",
    "    '''\n",
    "    # your code here\n",
    "    \n",
    "    vid_counts = events.groupby('pid')['vid'].nunique().reset_index()\n",
    "    vid_counts.columns = ['pid', 'encounter_count']\n",
    "\n",
    "    patient_df = vid_counts.merge(hf, on='pid', how='left')\n",
    "    normal_patients = patient_df[patient_df['label'].isna()]\n",
    "    hf_patients = patient_df[patient_df['label']==1]\n",
    "    \n",
    "    avg_hf_encounter_count = hf_patients['encounter_count'].mean() if not hf_patients.empty else None\n",
    "    max_hf_encounter_count = hf_patients['encounter_count'].max() if not hf_patients.empty else None\n",
    "    min_hf_encounter_count = hf_patients['encounter_count'].min() if not hf_patients.empty else None\n",
    "    avg_norm_encounter_count = normal_patients['encounter_count'].mean() if not normal_patients.empty else None\n",
    "    max_norm_encounter_count = normal_patients['encounter_count'].max() if not normal_patients.empty else None\n",
    "    min_norm_encounter_count = normal_patients['encounter_count'].min() if not normal_patients.empty else None\n",
    "    \n",
    "    return avg_hf_encounter_count, max_hf_encounter_count, min_hf_encounter_count, \\\n",
    "           avg_norm_encounter_count, max_norm_encounter_count, min_norm_encounter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events, hf = read_csv(TRAIN_DATA_PATH)\n",
    "\n",
    "#Compute the event count metrics\n",
    "start_time = time.time()\n",
    "event_count = event_count_metrics(events, hf)\n",
    "end_time = time.time()\n",
    "print((\"Time to compute event count metrics: \" + str(end_time - start_time) + \"s\"))\n",
    "print(event_count)\n",
    "\n",
    "#Compute the encounter count metrics\n",
    "start_time = time.time()\n",
    "encounter_count = encounter_count_metrics(events, hf)\n",
    "end_time = time.time()\n",
    "print((\"Time to compute encounter count metrics: \" + str(end_time - start_time) + \"s\"))\n",
    "print(encounter_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Feature construction [40 points] \n",
    "\n",
    "It is a common practice to convert raw data into a standard data format before running real machine learning models. In this question, you will implement the necessary python functions in this script. You will work with *events.csv*, *hf_events.csv* and *event_feature_map.csv* files provided in **TRAIN_DATA_PATH** folder. The use of `pandas` library in this question is recommended. \n",
    "\n",
    "Listed below are a few concepts you need to know before beginning feature construction (for details please refer to lectures). \n",
    "\n",
    "<img src=\"img/window.jpg\" width=\"600\"/>\n",
    "\n",
    "- **Index vid**: Index vid is evaluated as follows:\n",
    "  - For heart failure patients: Index vid is the vid of the first visit with heart failure for that patient (i.e., vid field in *hf_events.csv*). \n",
    "  - For normal patients: Index vid is the vid of the last visit for that patient (i.e., vid field in *events.csv*). \n",
    "- **Observation Window**: The time interval you will use to identify relevant events. Only events present in this window should be included while constructing feature vectors.\n",
    "- **Prediction Window**: A fixed time interval that is to be used to make the prediction.\n",
    "\n",
    "In the example above, the index vid is 3. Visits with vid 0, 1, 2 are within the observation window. The prediction window is between visit 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Compute the index vid [10 points]\n",
    "\n",
    "Use the definition provided above to compute the index vid for all patients. Complete the method `read_csv` and `calculate_index_vid` provided in the following code cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "\n",
    "def read_csv(filepath=TRAIN_DATA_PATH):\n",
    "    \n",
    "    '''\n",
    "    Read the events.csv, hf_events.csv and event_feature_map.csv files.\n",
    "    \n",
    "    NOTE: remember to use `filepath` whose default value is `TRAIN_DATA_PATH`.\n",
    "    '''\n",
    "\n",
    "    events = pd.read_csv(filepath + 'events.csv')\n",
    "    hf = pd.read_csv(filepath + 'hf_events.csv')\n",
    "    feature_map = pd.read_csv(filepath + 'event_feature_map.csv')\n",
    "\n",
    "    return events, hf, feature_map\n",
    "\n",
    "\n",
    "def calculate_index_vid(events, hf):\n",
    "    \n",
    "    '''\n",
    "    TODO: This function needs to be completed.\n",
    "\n",
    "    Suggested steps:\n",
    "        1. Create list of normal patients (hf_events.csv only contains information about heart failure patients).\n",
    "        2. Split events into two groups based on whether the patient has heart failure or not.\n",
    "        3. Calculate index vid for each patient.\n",
    "    \n",
    "    IMPORTANT:\n",
    "        `indx_vid` should be a pd dataframe with header `['pid', 'indx_vid']`.\n",
    "    '''\n",
    "    # Create list of normal patient\n",
    "    all_patients = set(events['pid'].unique())\n",
    "    hf_patients = set(hf['pid'].unique())\n",
    "    normal_patients = all_patients - hf_patients\n",
    "\n",
    "    # Split events into two groups based on whether the patient has heart failure or not\n",
    "    hf_events = events[events['pid'].isin(hf_patients)]\n",
    "    normal_events = events[events['pid'].isin(normal_patients)]\n",
    "\n",
    "    # Calculating Index for each group\n",
    "    hf_index_dict = hf_events.groupby('pid')['vid'].min().to_dict()\n",
    "    normal_index_dict = normal_events.groupby('pid')['vid'].max().to_dict()\n",
    "\n",
    "    # Combine both dict\n",
    "    combined_index_dict = {**hf_index_dict,**normal_index_dict}\n",
    "\n",
    "    # Convert to DataFrame:\n",
    "    indx_vid = pd.DataFrame(\n",
    "            list(combined_index_dict.items()),\n",
    "            columns=['pid', 'indx_vid']\n",
    "        )\n",
    "    \n",
    "    return indx_vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "events, hf, feature_map = read_csv(TRAIN_DATA_PATH)\n",
    "indx_vid_df = calculate_index_vid(events, hf)\n",
    "assert indx_vid_df.shape == (4000, 2), \"calculate_index_vid failed!\"\n",
    "\n",
    "indx_vid = dict(list(zip(indx_vid_df.pid, indx_vid_df.indx_vid)))\n",
    "assert indx_vid[78] == 1, \"calculate_index_vid failed!\"\n",
    "assert indx_vid[1230] == 5, \"calculate_index_vid failed!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Filter events [10 points]\n",
    "\n",
    "Remove the events that occur outside the observation window. That is, only keep events in visits before index vid. Complete the method *filter_events* provided in the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting indx_vid to dataframe again\n",
    "indx_vid = pd.DataFrame(\n",
    "        list(indx_vid.items()),\n",
    "        columns=['pid', 'indx_vid']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_events(events, indx_vid):\n",
    "    \n",
    "    '''\n",
    "    TODO: This function needs to be completed.\n",
    "\n",
    "    Suggested steps:\n",
    "        1. Join indx_vid with events on pid.\n",
    "        2. Filter events occuring in the observation window [:, index vid) (Note that the right side is OPEN).\n",
    "    \n",
    "    \n",
    "    IMPORTANT:\n",
    "        `filtered_events` should be a pd dataframe withe header  `['pid', 'event_id', 'value']`.\n",
    "    '''\n",
    "\n",
    "    # Join indx_vid with events on pid.\n",
    "    join_events = pd.merge(\n",
    "        events,\n",
    "        indx_vid,\n",
    "        on='pid',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Filter events occuring in the observation window [:, index vid) (Note that the right side is OPEN).\n",
    "    filtered_events = join_events[join_events['vid']<join_events['indx_vid']]\n",
    "    filtered_events = filtered_events[['pid', 'event_id', 'value']]\n",
    "    \n",
    "    return filtered_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "events, hf, feature_map = read_csv(TRAIN_DATA_PATH)\n",
    "indx_vid = calculate_index_vid(events, hf)\n",
    "filtered_events = filter_events(events, indx_vid)\n",
    "assert filtered_events[filtered_events.pid == 78].shape == (128, 3), \"filter_events failed!\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
