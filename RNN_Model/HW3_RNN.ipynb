{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3 _ RNN\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this homework, you will build a bi-directional RNN on diagnosis codes. The recurrent nature of RNN allows us to model the temporal relation of different visits of a patient. More specifically, we will still perform Heart Failure Prediction, but with different input formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# Get the current path:\n",
    "current_dir = os.path.dirname(os.path.abspath(''))\n",
    "\n",
    "# Define data path\n",
    "DATA_PATH = \"./HW3_RNN-lib/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Raw Data\n",
    "\n",
    "To get started, we will implement a naive RNN model for heart failure prediction using the diagnosis codes.\n",
    "\n",
    "We will use the same dataset synthesized from MIMIC-III, but with different input formats.\n",
    "\n",
    "The data has been preprocessed for you. Let us load them and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pids = pickle.load(open(os.path.join(DATA_PATH,'train/pids.pkl'), 'rb'))\n",
    "vids = pickle.load(open(os.path.join(DATA_PATH,'train/vids.pkl'), 'rb'))\n",
    "hfs = pickle.load(open(os.path.join(DATA_PATH,'train/hfs.pkl'), 'rb'))\n",
    "seqs = pickle.load(open(os.path.join(DATA_PATH,'train/seqs.pkl'), 'rb'))\n",
    "types = pickle.load(open(os.path.join(DATA_PATH,'train/types.pkl'), 'rb'))\n",
    "rtypes = pickle.load(open(os.path.join(DATA_PATH,'train/rtypes.pkl'), 'rb'))\n",
    "\n",
    "assert len(pids) == len(vids) == len(hfs) == len(seqs) == 1000\n",
    "assert len(types) == 619\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "\n",
    "- `pids`: contains the patient ids\n",
    "- `vids`: contains a list of visit ids for each patient\n",
    "- `hfs`: contains the heart failure label (0: normal, 1: heart failure) for each patient\n",
    "- `seqs`: contains a list of visit (in ICD9 codes) for each patient\n",
    "- `types`: contains the map from ICD9 codes to ICD-9 labels\n",
    "- `rtypes`: contains the map from ICD9 labels to ICD9 codes\n",
    "\n",
    "Let us take a patient as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient ID: 47537\n",
      "Heart Failure: 0\n",
      "# of visits: 2\n",
      "\t0-th visit id: 0\n",
      "\t0-th visit diagnosis labels: [12, 103, 262, 285, 290, 292, 359, 416, 39, 225, 275, 294, 326, 267, 93]\n",
      "\t0-th visit diagnosis codes: ['DIAG_041', 'DIAG_276', 'DIAG_518', 'DIAG_560', 'DIAG_567', 'DIAG_569', 'DIAG_707', 'DIAG_785', 'DIAG_155', 'DIAG_456', 'DIAG_537', 'DIAG_571', 'DIAG_608', 'DIAG_529', 'DIAG_263']\n",
      "\t1-th visit id: 1\n",
      "\t1-th visit diagnosis labels: [12, 103, 240, 262, 290, 292, 319, 359, 510, 513, 577, 307, 8, 280, 18, 131]\n",
      "\t1-th visit diagnosis codes: ['DIAG_041', 'DIAG_276', 'DIAG_482', 'DIAG_518', 'DIAG_567', 'DIAG_569', 'DIAG_599', 'DIAG_707', 'DIAG_995', 'DIAG_998', 'DIAG_V09', 'DIAG_584', 'DIAG_031', 'DIAG_553', 'DIAG_070', 'DIAG_305']\n"
     ]
    }
   ],
   "source": [
    "# take the 3rd patient as an example\n",
    "\n",
    "print(\"Patient ID:\", pids[3])\n",
    "print(\"Heart Failure:\", hfs[3])\n",
    "print(\"# of visits:\", len(vids[3]))\n",
    "for visit in range(len(vids[3])):\n",
    "    print(f\"\\t{visit}-th visit id:\", vids[3][visit])\n",
    "    print(f\"\\t{visit}-th visit diagnosis labels:\", seqs[3][visit])\n",
    "    print(f\"\\t{visit}-th visit diagnosis codes:\", [rtypes[label] for label in seqs[3][visit]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that seqs is a list of list of list. That is, seqs[i][j][k] gives you the k-th diagnosis codes for the j-th visit for the i-th patient.\n",
    "\n",
    "And you can look up the meaning of the ICD9 code online. For example, DIAG_276 represetns disorders of fluid electrolyte and acid-base balance.\n",
    "\n",
    "Further, let see number of heart failure patients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of heart failure patients: 548\n",
      "ratio of heart failure patients: 0.55\n"
     ]
    }
   ],
   "source": [
    "print(\"number of heart failure patients:\", sum(hfs))\n",
    "print(\"ratio of heart failure patients: %.2f\" % (sum(hfs) / len(hfs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Build the dataset \n",
    "\n",
    "### 1.1 CustomDataset \n",
    "\n",
    "First, let us implement a custom dataset using PyTorch class `Dataset`, which will characterize the key features of the dataset we want to generate.\n",
    "\n",
    "We will use the sequences of diagnosis codes `seqs` as input and heart failure `hfs` as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[[85, 112, 346, 380, 269, 511, 114, 103, 530, 597, 511], [85, 103, 112, 513, 511, 19, 149, 530, 186, 66]]], [1])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, seqs, hfs):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Store `seqs`. to `self.x` and `hfs` to `self.y`.\n",
    "        \n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        Do NOT permute the data.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.x = seqs\n",
    "        self.y = hfs\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Return the number of samples (i.e. patients).\n",
    "        \"\"\"\n",
    "        \n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Generates one sample of data.\n",
    "        \n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.x[index], self.y[index]\n",
    "        \n",
    "\n",
    "dataset = CustomDataset(seqs, hfs)\n",
    "\n",
    "print(dataset[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Collate Function [20 points]\n",
    "\n",
    "As you note that, we do not convert the data to tensor in the built `CustomDataset`. Instead, we will do this using a collate function `collate_fn()`. \n",
    "\n",
    "This collate function `collate_fn()` will be called by `DataLoader` after fetching a list of samples using the indices from `CustomDataset` to collate the list of samples into batches.\n",
    "\n",
    "For example, assume the `DataLoader` gets a list of two samples.\n",
    "\n",
    "```\n",
    "[ [ [0, 1, 2], [8, 0] ], \n",
    "  [ [12, 13, 6, 7], [12], [23, 11] ] ]\n",
    "```\n",
    "\n",
    "where the first sample has two visits `[0, 1, 2]` and `[8, 0]` and the second sample has three visits `[12, 13, 6, 7]`, `[12]`, and `[23, 11]`.\n",
    "\n",
    "The collate function `collate_fn()` is supposed to pad them into the same shape (3, 4), where 3 is the maximum number of visits and 4 is the maximum number of diagnosis codes.\n",
    "\n",
    "``` \n",
    "[ [ [0, 1, 2, *0*], [8, 0, *0*, *0*], [*0*, *0*, *0*, *0*]  ], \n",
    "  [ [12, 13, 6, 7], [12, *0*, *0*, *0*], [23, 11, *0*, *0*] ] ]\n",
    "```\n",
    "\n",
    "Further, the padding information will be stored in a mask with the same shape, where 1 indicates that the diagnosis code at this position is from the original input, and 0 indicates that the diagnosis code at this position is the padded value.\n",
    "\n",
    "```\n",
    "[ [ [1, 1, 1, 0], [1, 1, 0, 0], [0, 0, 0, 0] ], \n",
    "  [ [1, 1, 1, 1], [1, 0, 0, 0], [1, 1, 0, 0] ] ]\n",
    "```\n",
    "\n",
    "Lastly, we will have another diagnosis sequence in reversed time. This will be used in our RNN model for masking. Note that we only flip the true visits.\n",
    "\n",
    "``` \n",
    "[ [ [8, 0, *0*, *0*], [0, 1, 2, *0*], [*0*, *0*, *0*, *0*]  ], \n",
    "  [ [23, 11, *0*, *0*], [12, *0*, *0*, *0*], [12, 13, 6, 7] ] ]\n",
    "```\n",
    "\n",
    "And a reversed mask as well.\n",
    "\n",
    "```\n",
    "[ [ [1, 1, 0, 0], [1, 1, 1, 0], [0, 0, 0, 0] ], \n",
    "  [ [1, 1, 0, 0], [1, 0, 0, 0], [1, 1, 1, 1], ] ]\n",
    "```\n",
    "\n",
    "We need to pad the sequences into the same length so that we can do batch training on GPU. And we also need this mask so that when training, we can ignored the padded value as they actually do not contain any information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]])\n"
     ]
    }
   ],
   "source": [
    "data = CustomDataset(seqs, hfs)\n",
    "sequences, labels = zip(*data) \n",
    "\n",
    "num_patients = len(sequences)\n",
    "num_visits = [len(patient) for patient in sequences]\n",
    "num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "\n",
    "max_num_visits = max(num_visits)\n",
    "max_num_codes = max(num_codes)\n",
    "\n",
    "x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "        sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "        is stored in `mask`.\n",
    "    \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
    "        masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
    "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "        y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "        using: `sequences, labels = zip(*data)`\n",
    "    \"\"\"\n",
    "\n",
    "    sequences, labels = zip(*data)\n",
    "\n",
    "    y = torch.tensor(labels, dtype=torch.float)\n",
    "    \n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [len(patient) for patient in sequences]\n",
    "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "\n",
    "    max_num_visits = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    rev_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "\n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "        for j_visit, visit in enumerate(patient):\n",
    "            # Fill in the diagnosis codes for this visit\n",
    "            x[i_patient, j_visit, :len(visit)] = torch.tensor(visit)\n",
    "            # Create mask for valid positions (True where we have actual values)\n",
    "            masks[i_patient, j_visit, :len(visit)] = True\n",
    "            \n",
    "            # Fill in the reversed version\n",
    "            # Calculate reversed visit index\n",
    "            rev_j = max_num_visits - j_visit - 1\n",
    "            rev_x[i_patient, rev_j, :len(visit)] = torch.tensor(visit)\n",
    "            rev_masks[i_patient, rev_j, :len(visit)] = True\n",
    "    \n",
    "    return x, masks, rev_x, rev_masks, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=10, collate_fn=collate_fn)\n",
    "loader_iter = iter(loader)\n",
    "x, masks, rev_x, rev_masks, y = next(loader_iter)\n",
    "\n",
    "assert x.dtype == rev_x.dtype == torch.long\n",
    "assert y.dtype == torch.float\n",
    "assert masks.dtype == rev_masks.dtype == torch.bool\n",
    "\n",
    "assert x.shape == rev_x.shape == masks.shape == rev_masks.shape == (10, 3, 24)\n",
    "assert y.shape == (10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have `CustomDataset` and `collate_fn()`. Let us split the dataset into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 800\n",
      "Length of val dataset: 200\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "split = int(len(dataset)*0.8)\n",
    "\n",
    "lengths = [split, len(dataset) - split]\n",
    "train_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hf_events.csv**\n",
    "\n",
    "The data provided in *hf_events.csv* contains pid of patients who have been diagnosed with heart failure (i.e., DIAG_398, DIAG_402, DIAG_404, DIAG_428) in at least one visit. They are in the form of a tuple with the format *(pid, vid, label)*. For example,\n",
    "\n",
    "```\n",
    "156,0,1\n",
    "181,1,1\n",
    "```\n",
    "\n",
    "The vid indicates the index of the first visit with heart failure of that patient and a label of 1 indicates the presence of heart failure. **Note that only patients with heart failure are included in this file. Patients who are not mentioned in this file have never been diagnosed with heart failure.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**event_feature_map.csv**\n",
    "\n",
    "The *event_feature_map.csv* is a map from an event_id to an integer index. This file contains *(idx, event_id)* pairs for all event ids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Descriptive Statistics [20 points]\n",
    "\n",
    "Before starting analytic modeling, it is a good practice to get descriptive statistics of the input raw data. In this question, you need to write code that computes various metrics on the data described previously. A skeleton code is provided to you as a starting point.\n",
    "\n",
    "The definition of terms used in the result table are described below:\n",
    "\n",
    "- **Event count**: Number of events recorded for a given patient.\n",
    "- **Encounter count**: Number of visits recorded for a given patient.\n",
    "\n",
    "Note that every line in the input file is an event, while each visit consists of multiple events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete the following code cell to implement the required statistics.**\n",
    "\n",
    "Please be aware that **you are NOT allowed to change the filename and any existing function declarations.** Only `numpy`, `scipy`, `scikit-learn`, `pandas` and other built-in modules of python will be available for you to use. The use of `pandas` library is suggested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# PLEASE USE THE GIVEN FUNCTION NAME, DO NOT CHANGE IT.\n",
    "\n",
    "def read_csv(filepath=TRAIN_DATA_PATH):\n",
    "\n",
    "    '''\n",
    "    Read the events.csv and hf_events.csv files. \n",
    "    Variables returned from this function are passed as input to the metric functions.\n",
    "    \n",
    "    NOTE: remember to use `filepath` whose default value is `TRAIN_DATA_PATH`.\n",
    "    '''\n",
    "    \n",
    "    events = pd.read_csv(filepath + 'events.csv')\n",
    "    hf = pd.read_csv(filepath + 'hf_events.csv')\n",
    "\n",
    "    return events, hf\n",
    "\n",
    "def event_count_metrics(events, hf):\n",
    "\n",
    "    '''\n",
    "    TODO : Implement this function to return the event count metrics.\n",
    "    \n",
    "    Event count is defined as the number of events recorded for a given patient.\n",
    "    '''\n",
    "    ## your code here\n",
    "    \n",
    "    # Count events per patient\n",
    "    event_counts = events['pid'].value_counts().reset_index()\n",
    "    event_counts.columns = ['pid', 'event_count']\n",
    "    \n",
    "    # Merge event counts with HF status\n",
    "    patient_df = event_counts.merge(hf, on='pid', how='left')\n",
    "    normal_patients = patient_df[patient_df['label'].isna()]\n",
    "    hf_patients = patient_df[patient_df['label']==1]\n",
    "    \n",
    "    # Calculate metrics for HF patients\n",
    "    avg_hf_event_count = hf_patients['event_count'].mean() if not hf_patients.empty else None\n",
    "    max_hf_event_count = hf_patients['event_count'].max() if not hf_patients.empty else None\n",
    "    min_hf_event_count = hf_patients['event_count'].min() if not hf_patients.empty else None\n",
    "    \n",
    "    # Calculate metrics for normal patients\n",
    "    avg_norm_event_count = normal_patients['event_count'].mean() if not normal_patients.empty else None\n",
    "    max_norm_event_count = normal_patients['event_count'].max() if not normal_patients.empty else None\n",
    "    min_norm_event_count = normal_patients['event_count'].min() if not normal_patients.empty else None\n",
    "\n",
    "    return avg_hf_event_count, max_hf_event_count, min_hf_event_count, \\\n",
    "           avg_norm_event_count, max_norm_event_count, min_norm_event_count\n",
    "\n",
    "def encounter_count_metrics(events, hf):\n",
    "\n",
    "    '''\n",
    "    TODO : Implement this function to return the encounter count metrics.\n",
    "    \n",
    "    Encounter count is defined as the number of visits recorded for a given patient. \n",
    "    '''\n",
    "    # your code here\n",
    "    \n",
    "    vid_counts = events.groupby('pid')['vid'].nunique().reset_index()\n",
    "    vid_counts.columns = ['pid', 'encounter_count']\n",
    "\n",
    "    patient_df = vid_counts.merge(hf, on='pid', how='left')\n",
    "    normal_patients = patient_df[patient_df['label'].isna()]\n",
    "    hf_patients = patient_df[patient_df['label']==1]\n",
    "    \n",
    "    avg_hf_encounter_count = hf_patients['encounter_count'].mean() if not hf_patients.empty else None\n",
    "    max_hf_encounter_count = hf_patients['encounter_count'].max() if not hf_patients.empty else None\n",
    "    min_hf_encounter_count = hf_patients['encounter_count'].min() if not hf_patients.empty else None\n",
    "    avg_norm_encounter_count = normal_patients['encounter_count'].mean() if not normal_patients.empty else None\n",
    "    max_norm_encounter_count = normal_patients['encounter_count'].max() if not normal_patients.empty else None\n",
    "    min_norm_encounter_count = normal_patients['encounter_count'].min() if not normal_patients.empty else None\n",
    "    \n",
    "    return avg_hf_encounter_count, max_hf_encounter_count, min_hf_encounter_count, \\\n",
    "           avg_norm_encounter_count, max_norm_encounter_count, min_norm_encounter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events, hf = read_csv(TRAIN_DATA_PATH)\n",
    "\n",
    "#Compute the event count metrics\n",
    "start_time = time.time()\n",
    "event_count = event_count_metrics(events, hf)\n",
    "end_time = time.time()\n",
    "print((\"Time to compute event count metrics: \" + str(end_time - start_time) + \"s\"))\n",
    "print(event_count)\n",
    "\n",
    "#Compute the encounter count metrics\n",
    "start_time = time.time()\n",
    "encounter_count = encounter_count_metrics(events, hf)\n",
    "end_time = time.time()\n",
    "print((\"Time to compute encounter count metrics: \" + str(end_time - start_time) + \"s\"))\n",
    "print(encounter_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Naive RNN [35 points] \n",
    "\n",
    "Let us implement a naive bi-directional RNN model.\n",
    "\n",
    "<img src=\"img/bi-rnn.jpg\" width=\"600\"/>\n",
    "\n",
    "Remember from class that, first of all, we need to transform the diagnosis code for each visit of a patient to an embedding. To do this, we can use `nn.Embedding()`, where `num_embeddings` is the number of diagnosis codes and `embedding_dim` is the embedding dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can construct a simple RNN structure. Each input is this multi-hot vector. At the 0-th visit, this has $\\boldsymbol{X}_0$, and at t-th visit, this has $\\boldsymbol{X}_t$.\n",
    "\n",
    "Each one of the input will then map to a hidden state $\\boldsymbol{\\overleftrightarrow{h}}_t$. The forward hidden state $\\boldsymbol{\\overrightarrow{h}}_t$ can be determined by $\\boldsymbol{\\overrightarrow{h}}_{t-1}$ and the corresponding current input $\\boldsymbol{X}_t$.\n",
    "\n",
    "Similarly, we will have another RNN to process the sequence in the reverse order, so that the hidden state $\\boldsymbol{\\overleftarrow{h}}_t$ is determined by $\\boldsymbol{\\overleftarrow{h}}_{t+1}$ and $\\boldsymbol{X}_t$.\n",
    "\n",
    "Finally, once we have the $\\boldsymbol{\\overrightarrow{h}}_T$ and $\\boldsymbol{\\overleftarrow{h}}_{0}$, we will concatenate the two vectors as the feature vector and train a NN to perform the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us build this model. The forward steps will be:\n",
    "\n",
    "    1. Pass the sequence through the embedding layer;\n",
    "    2. Sum the embeddings for each diagnosis code up for a visit of a patient;\n",
    "    3. Pass the embeddings through the RNN layer;\n",
    "    4. Obtain the hidden state at the last visit;\n",
    "    5. Do 1-4 for both directions and concatenate the hidden states.\n",
    "    6. Pass the hidden state through the linear and activation layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Mask Selection [20 points]\n",
    "\n",
    "Importantly, you need to use `masks` to mask out the paddings in before step 2 and before 4. So, let us first preform the mask selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_embeddings_with_mask(x, masks):\n",
    "    \"\"\"\n",
    "    Sum embeddings for true visits using mask selection\n",
    "    \n",
    "    Arguments:\n",
    "        x: embeddings tensor (batch_size, # visits, # diagnosis codes, embedding_dim)\n",
    "        masks: boolean mask tensor (batch_size, # visits, # diagnosis codes)\n",
    "    \n",
    "    Returns:\n",
    "        sum_embeddings: summed embeddings (batch_size, # visits, embedding_dim)\n",
    "    \"\"\"\n",
    "    # Expand mask to match embedding dimensions\n",
    "    expanded_masks = masks.unsqueeze(-1).float()\n",
    "    \n",
    "    # Apply mask by multiplying with embeddings\n",
    "    # masked_x shape: (batch_size, # visits, # diagnosis codes, embedding_dim)\n",
    "    masked_x = x * expanded_masks\n",
    "    \n",
    "    # Sum along diagnosis codes dimension (dim=2)\n",
    "    # sum_embeddings shape: (batch_size, # visits, embedding_dim)\n",
    "    sum_embeddings = torch.sum(masked_x, dim=2)\n",
    "    \n",
    "    return sum_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shapes:\n",
      "x shape: torch.Size([2, 3, 4, 5])\n",
      "masks shape: torch.Size([2, 3, 4])\n",
      "Output shape: torch.Size([2, 3, 5])\n",
      "\n",
      "Verifying results:\n",
      "Manual sum for first visit: tensor([ 0.3717,  1.6714,  1.0547, -0.2694, -2.4057])\n",
      "Function result for first visit: tensor([ 0.3717,  1.6714,  1.0547, -0.2694, -2.4057])\n"
     ]
    }
   ],
   "source": [
    "def test_sum_embeddings_with_mask():\n",
    "    # Create sample data\n",
    "    batch_size = 2\n",
    "    num_visits = 3\n",
    "    num_codes = 4\n",
    "    embedding_dim = 5\n",
    "    \n",
    "    # Create sample embeddings\n",
    "    x = torch.randn(batch_size, num_visits, num_codes, embedding_dim)\n",
    "    \n",
    "    # Create sample masks\n",
    "    masks = torch.zeros(batch_size, num_visits, num_codes, dtype=torch.bool)\n",
    "    masks[:, :, :2] = True  # First two codes in each visit are valid\n",
    "    \n",
    "    # Apply function\n",
    "    result = sum_embeddings_with_mask(x, masks)\n",
    "    \n",
    "    # Print shapes and results\n",
    "    print(\"Input shapes:\")\n",
    "    print(f\"x shape: {x.shape}\")\n",
    "    print(f\"masks shape: {masks.shape}\")\n",
    "    print(f\"Output shape: {result.shape}\")\n",
    "    \n",
    "    # Verify results\n",
    "    print(\"\\nVerifying results:\")\n",
    "    # Manual calculation for first visit of first batch\n",
    "    manual_sum = x[0, 0, :2].sum(dim=0)  # Sum only first two codes\n",
    "    print(f\"Manual sum for first visit: {manual_sum}\")\n",
    "    print(f\"Function result for first visit: {result[0, 0]}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run test\n",
    "test_result = test_sum_embeddings_with_mask()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import ast\n",
    "import inspect\n",
    "\n",
    "\n",
    "def uses_loop(function):\n",
    "    loop_statements = ast.For, ast.While, ast.AsyncFor\n",
    "\n",
    "    nodes = ast.walk(ast.parse(inspect.getsource(function)))\n",
    "    return any(isinstance(node, loop_statements) for node in nodes)\n",
    "\n",
    "def generate_random_mask(batch_size, max_num_visits , max_num_codes):\n",
    "    num_visits = [random.randint(1, max_num_visits) for _ in range(batch_size)]\n",
    "    num_codes = []\n",
    "    for n in num_visits:\n",
    "        num_codes_visit = [0] * max_num_visits\n",
    "        for i in range(n):\n",
    "            num_codes_visit[i] = (random.randint(1, max_num_codes))\n",
    "        num_codes.append(num_codes_visit)\n",
    "    masks = [torch.ones((l,), dtype=torch.bool) for num_codes_visit in num_codes for l in num_codes_visit]\n",
    "    masks = torch.stack([torch.cat([i, i.new_zeros(max_num_codes - i.size(0))], 0) for i in masks], 0)\n",
    "    masks = masks.view((batch_size, max_num_visits, max_num_codes)).bool()\n",
    "    return masks\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "max_num_visits = 10\n",
    "max_num_codes = 20\n",
    "embedding_dim = 100\n",
    "\n",
    "torch.random.manual_seed(7)\n",
    "x = torch.randn((batch_size, max_num_visits , max_num_codes, embedding_dim))\n",
    "masks = generate_random_mask(batch_size, max_num_visits , max_num_codes)\n",
    "out = sum_embeddings_with_mask(x, masks)\n",
    "\n",
    "assert uses_loop(sum_embeddings_with_mask) is False\n",
    "assert out.shape == (batch_size, max_num_visits, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_visit(hidden_states, masks):\n",
    "    \"\"\"\n",
    "    TODO: obtain the hidden state for the last true visit (not padding visits)\n",
    "\n",
    "    Arguments:\n",
    "        hidden_states: the hidden states of each visit of shape (batch_size, # visits, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        last_hidden_state: the hidden state for the last true visit of shape (batch_size, embedding_dim)\n",
    "        \n",
    "    NOTE: DO NOT use for loop.\n",
    "    \n",
    "    HINT: First convert the mask to a vector of shape (batch_size,) containing the true visit length; \n",
    "          and then use this length vector as index to select the last visit.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get any non-zero values along the diagnosis codes dimension to identify valid visits\n",
    "    # Shape: (batch_size, # visits)\n",
    "    valid_visits = masks.any(dim=-1)\n",
    "    \n",
    "    # Get the indices of the last true visit for each batch\n",
    "    # Shape: (batch_size,)\n",
    "    last_visit_indices = valid_visits.sum(dim=1) - 1\n",
    "    \n",
    "    # Create batch indices\n",
    "    # Shape: (batch_size,)\n",
    "    batch_indices = torch.arange(hidden_states.size(0))\n",
    "    \n",
    "    # Get the last hidden state for each sequence in the batch\n",
    "    # Shape: (batch_size, embedding_dim)\n",
    "    last_hidden_state = hidden_states[batch_indices, last_visit_indices]\n",
    "    \n",
    "    return last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert uses_loop(get_last_visit) is False\n",
    "\n",
    "max_num_visits = 10\n",
    "batch_size = 16\n",
    "max_num_codes = 20\n",
    "embedding_dim = 100\n",
    "\n",
    "torch.random.manual_seed(7)\n",
    "hidden_states = torch.randn((batch_size, max_num_visits, embedding_dim))\n",
    "masks = generate_random_mask(batch_size, max_num_visits , max_num_codes)\n",
    "out = get_last_visit(hidden_states, masks)\n",
    "\n",
    "assert out.shape == (batch_size, embedding_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
